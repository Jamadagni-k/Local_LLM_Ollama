# Local_LLM_Ollama
This repo is for local LLM's which use Ollama from the terminal as a back-end and Streamlit as the front end for using it as a local LLM
Download Ollama from https://ollama.ai/download
Download any model you want to use from https://ollama.ai/library
Run Ollama with the model you want to use as a back-end
Edit the model of your choice which you downloaded from the Ollama library and rename the model name in line no 23 in the python file.
This ensures the streamlit app is running the appropriate model which you downloaded from Ollama library.
Run the streamlit app using the command "streamlit run Local_LLM_Ollama.py"
Start using it from your local sever which is running on your local browser.
