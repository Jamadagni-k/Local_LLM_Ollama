# Local_LLM_Ollama
This repo is for local LLM's which use Ollama from the terminal as a back-end and Streamlit as the front end for using it as a local LLM
